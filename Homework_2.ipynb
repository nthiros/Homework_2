{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 (Due Sep. 27) \n",
    "Problems marked with a (\\*) are only required for graduate students.  Undergrads may undertake them for extra credit worth half the problems point value, with no penalties incurred for an incorrect answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicholas Thiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Networks/Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider three binary variables $a, b, c \\in \\{0, 1\\}$ having the joint distribution given by  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#        | a | b | c | P(a,b,c)\n",
    "P_table = (\n",
    "np.array([[0,  0,  0,  0.192 ],\n",
    "          [0,  0,  1,  0.144 ],\n",
    "          [0,  1,  0,  0.048 ],\n",
    "          [0,  1,  1,  0.216 ],\n",
    "          [1,  0,  0,  0.192 ],\n",
    "          [1,  0,  1,  0.064 ],\n",
    "          [1,  1,  0,  0.048 ],\n",
    "          [1,  1,  1,  0.096 ]]))\n",
    "\n",
    "# Convert P_table (which is a probability table) to a probability array, which \n",
    "# is much more computationally sensible for a discrete valued distribution\n",
    "P_abc = np.zeros((2,2,2))\n",
    "for entry in P_table:\n",
    "    a = int(entry[0])\n",
    "    b = int(entry[1])\n",
    "    c = int(entry[2])\n",
    "    P = entry[3]\n",
    "    P_abc[a,b,c] = P\n",
    "    \n",
    "#print(P_abc[0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you may want to use the following functions to help you in the following problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalize(P,variables_to_marginalize=(),keepdims=True):\n",
    "    \"\"\" Marginalize a probability table, i.e. compute P(a,b) = sum_c P(a,b,c) \n",
    "    \n",
    "        Arguments: \n",
    "        \n",
    "        P (array) -> a probability array in which the dimensions correspond to random variables,\n",
    "                     the indices to variable values, and the entries to probabilities\n",
    "        variables_to_marginalize (tuple) -> a list of integers containing the \n",
    "                                    variable numbers to marginalize over\n",
    "        keepdims (boolean) -> Marginalization reduces the dimensionality of the distribution.  keepdims=False\n",
    "                       removes that dimension from the array indexing scheme.  For example P(a,b,c) has\n",
    "                       an array that is 2x2x2.  If we marginalize over variable/index 1 (aka b), then the \n",
    "                       resulting array P(a,c) is 2x2 if keepdims=False (which may change the index of a \n",
    "                       given variable.  For example, after marginalizing, variable/index 1 is now c).  \n",
    "                       Conversely, the array becomes 2x1x2 if keepdims=True.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Sum over the axes given in variables_to_marginalize\n",
    "    return np.sum(P,axis=variables_to_marginalize,keepdims=keepdims)\n",
    "\n",
    "def condition(P,variables_to_condition=()):\n",
    "    \"\"\" Condition a probability table, i.e. compute P(a|c) = P(a,c)/P(c) \n",
    "    \n",
    "        Arguments: \n",
    "        \n",
    "        P (array) -> a probability array in which the dimensions correspond to random variables,\n",
    "                     the indices to variable values, and the entries to probabilities\n",
    "        variables_to_condition (tuple) -> a list of integers containing the variables to condition on\n",
    "        \n",
    "        NOTE: This function always returns an array that is the same size as the input.  However, this array\n",
    "        no longer sums to one as does an unconditioned joint distribution.  Instead, it sums to one only given\n",
    "        a particular value of the conditioned variable/index.  For example:\n",
    "        \n",
    "        P_abc.sum()==1 -> True\n",
    "        P_ab_given_c = condition(P_abc,variables_to_condition=(2,))\n",
    "        P_ab_given_c.sum()==1 -> False\n",
    "        P_ab_given_c[:,:,0].sum()==1 -> True\n",
    "        P_ab_given_c[:,:,1].sum()==1 -> True\n",
    "   \n",
    "    \"\"\"\n",
    "    # find the variables to marginalize over to get the marginal distribuion of the\n",
    "    # variables that we wish to condition on\n",
    "    v_to_m = list(range(P.ndim))\n",
    "    for v in variables_to_condition:\n",
    "        v_to_m.remove(v)\n",
    "    # Compute the conditional by dividing the input by the output\n",
    "    return P/marginalize(P,variables_to_marginalize=tuple(v_to_m),keepdims=True)\n",
    "    \n",
    "P_ab = marginalize(P_abc,variables_to_marginalize=(2,)) #would this be P_bc? We are marginilzing over index 0, which is 'a'\n",
    "P_a_given_b = condition(P_ab,variables_to_condition=(1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint distribution equivalency (20 pts)\n",
    "Show by direct evaluation that $P(a,b,c) = P(a)P(c|a)P(b|c)$ for all values of $a$,$b$, and $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Probability\n",
      " [[[0.192 0.144]\n",
      "  [0.048 0.216]]\n",
      "\n",
      " [[0.192 0.064]\n",
      "  [0.048 0.096]]]\n",
      "\n",
      "Original Joint Probabalities:\n",
      " [0.192 0.144 0.048 0.216 0.192 0.064 0.048 0.096]\n"
     ]
    }
   ],
   "source": [
    "P_a = marginalize(P_abc, variables_to_marginalize=(1,2))\n",
    "P_ca = marginalize(P_abc, variables_to_marginalize=(1))\n",
    "P_bc = marginalize(P_abc, variables_to_marginalize=(0))\n",
    "\n",
    "P_c_given_a = condition(P_ca, variables_to_condition=(0,))\n",
    "P_b_given_c = condition(P_bc, variables_to_condition=(2,))\n",
    "\n",
    "P_a_b_c = P_a * P_c_given_a * P_b_given_c\n",
    "\n",
    "print('Joint Probability\\n',P_a_b_c)\n",
    "print('\\nOriginal Joint Probabalities:\\n',P_table[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probability, which was obtained by applying the chain rule, is equivalent to the full joint probability table that is given.  This indicates that, indeed, this factorization is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization (10 pts)\n",
    "\n",
    "Visualize the corresponding directed graph, either using graph software like [networkx](https://networkx.github.io/) or drawing it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "rvs = ['A','B','C']\n",
    "\n",
    "G.add_nodes_from(rvs)\n",
    "\n",
    "G.add_edges_from([('B', 'C'),('C','A')], weight=2)\n",
    "nx.draw_networkx(G, arrows=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Trick Question in Graphs (\\*) (10 pts)\n",
    "Plot the graph associated with\n",
    "$$\n",
    "P(A,B,C,D,E) = P(A)P(C)P(B|A,C)P(D|C)P(E|D),\n",
    "$$\n",
    "where $A,B,C,D,E \\in \\{0,1\\}$ with conditional probability tables\n",
    "\\begin{align}\n",
    "P(A=1) &= 0.3 \\nonumber \\\\\n",
    "P(C=1) &= 0.7 \\nonumber \\\\\n",
    "P(B=1|A,C) &= \\begin{cases} 0.3\\,\\mathrm{if}\\,A=0,C=0\\\\\n",
    "                          0.7\\,\\mathrm{if}\\,A=1,C=0\\\\\n",
    "                          0.1\\,\\mathrm{if}\\,A=0,C=1\\\\\n",
    "                          0.9\\,\\mathrm{if}\\,A=1,C=1 \\end{cases} \\nonumber \\\\\n",
    "P(D=1|C) &= \\begin{cases}   0.4\\,\\mathrm{if}\\,C=0\\\\\n",
    "                          0.2\\,\\mathrm{if}\\,C=1 \\end{cases} \\nonumber \\\\\n",
    "P(E=1|D) &= \\begin{cases}   0.1\\,\\mathrm{if}\\,D=0\\\\\n",
    "                          0.1\\,\\mathrm{if}\\,D=1 \\end{cases}. \\nonumber\n",
    "\\end{align}\n",
    "Compute the probability $P(A=1|E=1,C=1)$.  Be sure to think carefully about conditional independence before pulling out your calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "rvs = ['A','B','C','D','E']\n",
    "\n",
    "G.add_nodes_from(rvs)\n",
    "\n",
    "G.add_edges_from([('C', 'B'),('A','B'),('C','D'),('D','E')], weight=2)\n",
    "nx.draw_networkx(G, arrows=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the graph, the probability of A is independent of the probabilities of C and E.  Hence, $$\\mathrm{P(A=1|E=1,C=1)=P(A=1)=0.3}$$ This is identical to its original probability of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Gene Sequence Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Training a Markov model (40 pts)\n",
    "Load the file genes\\_training.p, which is available in this homework archive.  genes\\_training.p contains 2000 sequences, with each sequence $\\mathbf{s}$ consisting of 20 nucleobases $s_i \\in \\mathrm{Nu},\\; \\mathrm{Nu} = \\{A,T,G,C\\}$.  Each of these sequences is generated from one of two separate Markov processes.  The label (aka class) of the process that generated the sequence is given in the dataset. \n",
    "\n",
    "Learn the Markov model for each class given the training data.  **To do this, for each class compute the prior probability $\\mathbf{\\pi}_c$ of each nucleobase (the relative frequency of each nucleobase for each class, a vector of length 4) and the matrix of transition probabilities** \n",
    "$$\n",
    "\\mathcal{A}_{c,kj} = P(s_{i+1}=\\mathrm{Nu}_j|s_{i}=\\mathrm{Nu}_k),\n",
    "$$ \n",
    "which is a 4 by 4 matrix.  As a quick sanity check, each row of $\\mathcal{A}_c$ should sum to one.  **Using these priors and transition matrices, write a function that generates a new sequence given the class**, i.e. simulates a data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set 0 priors [A T G C]\n",
      " [0.28145 0.26535 0.31195 0.14125]\n",
      "\n",
      "Training set 1 priors [A T G C]\n",
      " [0.332  0.1008 0.1901 0.3771]\n",
      "\n",
      "Training set 0 transitions [[AA AT AG AC][TA TT TG TC][GA GT GG GC][CA CT CG CC]]\n",
      " [[0.22677903 0.31910112 0.42771536 0.02640449]\n",
      " [0.20329889 0.17786169 0.26331479 0.35552464]\n",
      " [0.44653982 0.30527025 0.24381209 0.00437784]\n",
      " [0.17069543 0.24581629 0.31349944 0.26998884]]\n",
      "\n",
      "Training set 1 transitions [[AA AT AG AC][TA TT TG TC][GA GT GG GC][CA CT CG CC]]\n",
      " [[0.14585319 0.04671115 0.34429616 0.4631395 ]\n",
      " [0.24204486 0.23213354 0.40114763 0.12467397]\n",
      " [0.67282249 0.18467475 0.06174201 0.08076075]\n",
      " [0.35344226 0.0474794  0.04510543 0.55397291]]\n",
      "\n",
      "Simulated Sequence 0\n",
      " ['G' 'G' 'A' 'T' 'C' 'G' 'A' 'T' 'T' 'G' 'A' 'T' 'C' 'G' 'A' 'A' 'T' 'G'\n",
      " 'A' 'G']\n",
      "\n",
      "Simulated Sequence 1\n",
      " ['A' 'G' 'A' 'G' 'A' 'G' 'G' 'T' 'T' 'T' 'T' 'G' 'A' 'C' 'C' 'C' 'C' 'C'\n",
      " 'G' 'T']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define some useful constants\n",
    "N_nucleobases = 4\n",
    "N_classes = 2\n",
    "nucleobases = ['A','T','G','C']\n",
    "\n",
    "# Load the training data using pickle\n",
    "sequences,labels = pickle.load(open('genes_training.p','rb'))\n",
    "\n",
    "\n",
    "# Initialize the class priors and transition matrices\n",
    "pi_0 = np.zeros((N_nucleobases))\n",
    "pi_1 = np.zeros((N_nucleobases))\n",
    "\n",
    "A_0 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "A_1 = np.zeros((N_nucleobases,N_nucleobases))\n",
    "\n",
    "\n",
    "##### Train prior #####\n",
    "\n",
    "# Loop over all of the sequences and labels\n",
    "\n",
    "for s,l in zip(sequences,labels):\n",
    "    sequence_length = len(s)\n",
    "    for p in range(sequence_length):\n",
    "        if l == 0:\n",
    "            if s[p]=='A':\n",
    "                pi_0[0] += 1\n",
    "            elif s[p]=='T':\n",
    "                pi_0[1] += 1\n",
    "            elif s[p]=='G':\n",
    "                pi_0[2] += 1\n",
    "            elif s[p]=='C':\n",
    "                pi_0[3] += 1\n",
    "        if l == 1:\n",
    "            if s[p]=='A':\n",
    "                pi_1[0] += 1\n",
    "            elif s[p]=='T':\n",
    "                pi_1[1] += 1\n",
    "            elif s[p]=='G':\n",
    "                pi_1[2] += 1\n",
    "            elif s[p]=='C':\n",
    "                pi_1[3] += 1\n",
    "\n",
    "# Convert from counts to probabilities by normalizing\n",
    "pi_0/=pi_0.sum()\n",
    "print('\\nTraining set 0 priors [A T G C]\\n',(pi_0))\n",
    "pi_1/=pi_1.sum()\n",
    "print('\\nTraining set 1 priors [A T G C]\\n',pi_1)\n",
    "\n",
    "##### Train transition matrix #####\n",
    "for s,l in zip(sequences,labels):\n",
    "    sequence_length = len(s)\n",
    "    for p in range(sequence_length-1):\n",
    "        if l == 0:\n",
    "            for i in range(len(nucleobases)):\n",
    "                if s[p] == nucleobases[i]:\n",
    "                    if s[p+1] == 'A':\n",
    "                        A_0[i,0] += 1\n",
    "                    if s[p+1] == 'T':\n",
    "                        A_0[i,1] += 1\n",
    "                    if s[p+1] == 'G':\n",
    "                        A_0[i,2] += 1\n",
    "                    if s[p+1] == 'C':\n",
    "                        A_0[i,3] += 1\n",
    "        if l == 1:\n",
    "            for i in range(4):\n",
    "                if s[p] == nucleobases[i]:\n",
    "                    if s[p+1] == 'A':\n",
    "                        A_1[i,0] += 1\n",
    "                    if s[p+1] == 'T':\n",
    "                        A_1[i,1] += 1\n",
    "                    if s[p+1] == 'G':\n",
    "                        A_1[i,2] += 1\n",
    "                    if s[p+1] == 'C':\n",
    "                        A_1[i,3] += 1\n",
    "                            \n",
    "\n",
    "# Convert from counts to probabilities by row normalization\n",
    "A_0/=A_0.sum(axis=1)[:,np.newaxis]\n",
    "print('\\nTraining set 0 transitions [[AA AT AG AC][TA TT TG TC][GA GT GG GC][CA CT CG CC]]\\n',A_0)\n",
    "A_1/=A_1.sum(axis=1)[:,np.newaxis]\n",
    "print('\\nTraining set 1 transitions [[AA AT AG AC][TA TT TG TC][GA GT GG GC][CA CT CG CC]]\\n',A_1)\n",
    "\n",
    "#print(np.sum(A_0[0,:]))\n",
    "#print(np.sum(A_0[1,:]))\n",
    "#print(np.sum(A_0[2,:]))\n",
    "#print(np.sum(A_0[3,:]))\n",
    "\n",
    "\n",
    "##### Generate a synthetic sequence #####\n",
    "def generate_new_sequence(A,pi,n=20):\n",
    "    \"\"\"  \n",
    "    Arguments:\n",
    "    A -> Nucleobase transition matrix\n",
    "    pi -> Prior\n",
    "    n -> length of sequence to generate\n",
    "    \"\"\"\n",
    "    # Draw from the prior for the first nucleobase\n",
    "    #s = (np.random.choice(nucleobases,1,p=pi)).astype(str)\n",
    "    #seq = np.zeros(n).astype(str)\n",
    "    #seq = np.chararray((20))\n",
    "    #seq[0] = str(s)\n",
    "    #print(seq)\n",
    "    \n",
    "    s = (np.random.choice(len(nucleobases),1,p=pi))\n",
    "    seq = np.zeros(n)\n",
    "    seq[0] = int(s)\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        #need to grab the row from A matrix that corresonds to current nucleobase\n",
    "        #0=A, 1=T, 2=G 3=C\n",
    "        next_base_prob = (A[int(seq[i]),:]) \n",
    "        #print(next_base_prob)\n",
    "        next_base = (np.random.choice(len(nucleobases), 1, p=next_base_prob))\n",
    "        seq[i+1] = int(next_base)\n",
    "  \n",
    "    seq = seq.astype(str)\n",
    "    \n",
    "    for j in range(len(seq)):\n",
    "        if seq[j] == str(0.0):\n",
    "            seq[j] = str('A')\n",
    "        elif seq[j] == str(1.0):\n",
    "            seq[j] = str('T')\n",
    "        elif seq[j] == str(2.0):\n",
    "            seq[j] = str('G')\n",
    "        elif seq[j] == str(3.0):\n",
    "            seq[j] = str('C')\n",
    "    \n",
    "    return seq\n",
    "\n",
    "seq_ts0 = generate_new_sequence(A_0, pi_0, n=20)\n",
    "print('\\nSimulated Sequence 0\\n',seq_ts0)\n",
    "seq_ts1 = generate_new_sequence(A_1, pi_1, n=20)\n",
    "print('\\nSimulated Sequence 1\\n', seq_ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set priors represent the total frequency of each nucleotide in the training set divided by the total number of nucleotides.  In training set 0, A T and G should appear about equally with C appearing less.  Training set 1 is skewed to have a higher probability of A and C occurring.  \n",
    "\n",
    "The training set transition matrix represents the frequency of a nucleotide transitioning to another nucleotide.  This is then normalized by the total number of transitions for a given nucleotide.  In the matrices above, the first row is nucleotide A transitioning to A T G C, respectively (represented by the rows).  Row 2, 3, and 4, represent the T, G, and C transitions, repectively. What stands out in both of these transitions’ matrix is that transition from G to C is very unlikely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. A Markov classifier (*) (20 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the prior and transition probabilities gives you the ability to evaluate the likelihood of a sequence for a given class as:\n",
    "$$\n",
    "P(\\mathbf{s}) = P(s_1|\\mathbf{\\pi}_c) \\prod_{i=1}^{n-1} P(s_{i+1}|s_{i},\\mathcal{A}_c),\n",
    "$$\n",
    "where $\\mathbf{\\pi}_c$ is the class-conditioned prior probability, and $\\mathcal{A}_c$ is the class-conditioned transition matrix.  Comparing the computed likelihood for a given sequence between different classes forms the basis of a classifier in a very similar manner to naive Bayes.  The difference this time is that now each random variable depends on the one before it in the sequence, whereas in naive Bayes we assumed that all the random variables were independent.    \n",
    "\n",
    "Load the file genes\\_test.p, which is similar to genes\\_training.p.  **For each sequence, compute the likelihood for both classes and assign a label.  Compare this predicted label to the known one, and report the test set accuracy**.  As a point of comparison, my implementation achieved 98.7\\% accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test,labels_test = pickle.load(open('genes_test.p','rb'))\n",
    "\n",
    "def mapper(seq_list):\n",
    "    \"\"\"turns sequence of nucleobases into numbers that match the indexing of the transition matrices\n",
    "    A = 0, T = 1, G =2, C = 3\n",
    "    \"\"\"\n",
    "    nrows = len(seq_list)\n",
    "    ncols = len(seq_list[0])\n",
    "    sequence_nums = np.zeros((int(nrows), int(ncols)),dtype=int)\n",
    "    \n",
    "    for i in range(len(seq_list)):\n",
    "        for j in range(len(seq_list[0])):\n",
    "            if seq_list[i][j] == 'A':\n",
    "                sequence_nums[i][j] = int(0)\n",
    "            if seq_list[i][j] == 'T':\n",
    "                sequence_nums[i][j] = int(1)\n",
    "            if seq_list[i][j] == 'G':\n",
    "                sequence_nums[i][j] = int(2)\n",
    "            if seq_list[i][j] =='C':\n",
    "                sequence_nums[i][j] = int(3)\n",
    "    return sequence_nums\n",
    "               \n",
    "def get_likelihood(sequence, trans_mat):\n",
    "    \"\"\"sequence is the sequence of nucleobases in ATGC\n",
    "    trans_mat is the trasition matrix from the training set\n",
    "    \n",
    "    This function evaluates the likelihood that a given sequence of nucleobases came from a the given traning set class\n",
    "    It does this by multiplying the probability of each transition that occured together\n",
    "    Hence, each sequence of the 20 nucleobases has a particular likelihood of being part of class 0 or 1\"\"\"\n",
    "    sequence_nums = mapper(sequence)\n",
    "    likelihood = np.ones(len(sequence_nums))\n",
    "\n",
    "    for s in range(len(sequence_nums)): #iterate through each sequence\n",
    "        for p in range(len(sequence_nums[0])-1): #iterate through each nucleobase in a sequence\n",
    "            first_index = sequence_nums[s][p]\n",
    "            second_index = sequence_nums[s][p+1]\n",
    "            A = trans_mat[first_index, second_index]\n",
    "            #print (A, [first_index,second_index])\n",
    "            likelihood[s] *= A\n",
    "    return likelihood\n",
    "\n",
    "def get_prior(sequence, pri_array):\n",
    "    \"\"\"get the prior of for the first nucleobase based on each class of training set\"\"\"\n",
    "    prior = np.array([])\n",
    "    for s in sequence:\n",
    "        for i in range(len(nucleobases)):\n",
    "            if s[0] == nucleobases[i]:\n",
    "                pri = pi_0[i]\n",
    "                prior = np.append(prior, pri)\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences:  1000\n",
      "Number of sequences correctly classified:  988\n",
      "Accuracy = 98.8 percent\n"
     ]
    }
   ],
   "source": [
    "#likelhood and prior for the test sequences belonging to class 0\n",
    "likelihood_0 = get_likelihood(sequences_test, A_0)\n",
    "prior_0 = get_prior(sequences_test, pi_0)\n",
    "#likelihood and prior for the test sequences belonging to class 1\n",
    "likelihood_1 = get_likelihood(sequences_test, A_1)\n",
    "prior_1 = get_prior(sequences_test, pi_1)\n",
    "\n",
    "#probability of a sequence belogning to class 0 or 1 is the likelihood multiplied by the prior\n",
    "sequence_probabilities_0 = [prior_0*likelihood_0]\n",
    "sequence_probabilities_1 = [prior_1*likelihood_1]\n",
    "\n",
    "#get the odds ratio by taking the log of the probability ratio\n",
    "log_like_ratio = np.log(sequence_probabilities_0) - np.log(sequence_probabilities_1)\n",
    "\n",
    "#get the predicted classification (0 or 1) of the sequences\n",
    "group = np.zeros(len(sequences_test)).astype(int)\n",
    "for i in range(len(log_like_ratio[0])):\n",
    "    if log_like_ratio[0][i] < 0:\n",
    "        group[i] = int(1)               \n",
    "\n",
    "#compare classification to actual sequence labels\n",
    "count = 0\n",
    "for z in range(len(group)):\n",
    "    if int(group[z]) == int(labels_test[z]):\n",
    "        count += 1\n",
    "\n",
    "accuracy = float(count)/len(labels_test)*100\n",
    "\n",
    "\n",
    "print ('Total number of sequences: ', len(labels_test))\n",
    "print ('Number of sequences correctly classified: ', count)\n",
    "print ('Accuracy = %.1f percent'  % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 98.8% indicates that we correctly identified which class (0 or 1) produced the test sequences 988 out of the 1000 sequences.  This is remarkably high given this was just a first order Markov model, meaning we only had a 1 step memory.  This also seems remarkable given that (based on visual inspection) the training data priors and transitions matrices do not seem incredibly different and we were only considering a length 20 sequence.  I would expect that if longer strings were considered the statistics would be more robust and the accuracy would increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
